<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://devnithw.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://devnithw.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-23T05:32:45+00:00</updated><id>https://devnithw.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal portfolio including blog, projects and vitae. </subtitle><entry><title type="html">Mankind’s Last Invention — Superintelligence, Singularity and Intelligence Explosion</title><link href="https://devnithw.github.io/blog/2023/agi-medium/" rel="alternate" type="text/html" title="Mankind’s Last Invention — Superintelligence, Singularity and Intelligence Explosion"/><published>2023-01-07T00:01:00+00:00</published><updated>2023-01-07T00:01:00+00:00</updated><id>https://devnithw.github.io/blog/2023/agi-medium</id><content type="html" xml:base="https://devnithw.github.io/blog/2023/agi-medium/"><![CDATA[<p>Redirecting to medium.</p>]]></content><author><name></name></author><category term="article"/><summary type="html"><![CDATA[Discussion about the future of AGI]]></summary></entry><entry><title type="html">The Renaissance of AI Art in 2022</title><link href="https://devnithw.github.io/blog/2022/aiart-medium/" rel="alternate" type="text/html" title="The Renaissance of AI Art in 2022"/><published>2022-12-08T00:01:00+00:00</published><updated>2022-12-08T00:01:00+00:00</updated><id>https://devnithw.github.io/blog/2022/aiart-medium</id><content type="html" xml:base="https://devnithw.github.io/blog/2022/aiart-medium/"><![CDATA[<p>Redirecting to medium.</p>]]></content><author><name></name></author><category term="article"/><summary type="html"><![CDATA[The history of AI art and how diffusion models are taking over]]></summary></entry><entry><title type="html">Blood cell image classification [Computer Vision]</title><link href="https://devnithw.github.io/blog/2022/bloodcells/" rel="alternate" type="text/html" title="Blood cell image classification [Computer Vision]"/><published>2022-08-02T12:57:00+00:00</published><updated>2022-08-02T12:57:00+00:00</updated><id>https://devnithw.github.io/blog/2022/bloodcells</id><content type="html" xml:base="https://devnithw.github.io/blog/2022/bloodcells/"><![CDATA[<p><code class="language-plaintext highlighter-rouge">This notebook was riginally published on Kaggle. Therefore it requires datasets which were directly imported from Kaggle datasets.</code></p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/blood.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="tutorial"/><category term="kaggle"/><category term="deep-learning"/><category term="computer-vision"/><category term="jupyter"/><summary type="html"><![CDATA[Classifying blood cell images using a Convolutional Neural Network]]></summary></entry><entry><title type="html">Mel Frequency Cepstral Coefficients</title><link href="https://devnithw.github.io/blog/2022/mfcc/" rel="alternate" type="text/html" title="Mel Frequency Cepstral Coefficients"/><published>2022-08-02T12:57:00+00:00</published><updated>2022-08-02T12:57:00+00:00</updated><id>https://devnithw.github.io/blog/2022/mfcc</id><content type="html" xml:base="https://devnithw.github.io/blog/2022/mfcc/"><![CDATA[<h2 id="mfccs--looking-beyond-the-spectrum-of-human-speech">MFCCs : Looking Beyond the Spectrum of Human Speech</h2> <p><code class="language-plaintext highlighter-rouge">This article was originally published on ECarrier Magazine April 2024 Issue, the official magazine of the Department of Electronic and Telecommunication Engineering, University of Moratuwa. View the magazine [here](https://ent.uom.lk/e-carrier-magazine/) </code></p> <p>The Fourier Transform, a convenient technique to determine the frequency components of a signal, serves as one of the primary methods to extract features of an audio signal. However, with the advancement of the signal processing domain, more sophisticated methods such as short-time Fourier transform (STFT) and wavelet transform have been developed. In this article, we demonstrate one such evolved technique named “mel-frequency cepstral coefficients” (MFCC), which is adapted particularly for speech recognition tasks as a competent feature extractor. Furthermore, we delve into an effective application of this technique for speaker recognition at the IEEE Signal Processing Cup 2024 competition.</p> <h2 id="feature-extractors-the-datasheet-of-signals">Feature Extractors: The Datasheet of Signals</h2> <p>Before tackling what MFCC practically means or its derivation, we first consider the necessity of such a tool. Simply put, a feature extractor identifies certain qualities of a signal, similar to how characteristics like age, height, weight and gender describe a human. They extract key features of an audio signal, like its power, frequencies and rate of change. A trivially simple feature extractor of an audio signal is its root mean square value. This tells us about the power contained within the signal. The zero-crossing rate is another such primary feature. This is a measurement of how fast the signal changes. It is a widely used metric in speech recognition and music information retrieval. Both these tools extract features of the signal in the time domain. However, integral transforms such as the Fourier transform (FT) enable us to investigate the domain of frequency, which unravels new features. The FT, or its primary derivative, power spectral density, encapsulates entirely the frequency components included in the signal and their relative power. However, by converting to the frequency domain, we lose temporal information. As a solution, we mediate the trade-off and localise the signal both in time and frequency by taking the STFT. The STFT, commonly known as the spectrogram, of a speech signal is shown in Figure 1.</p>]]></content><author><name></name></author><category term="tutorial"/><category term="kaggle"/><category term="deep-learning"/><category term="computer-vision"/><category term="jupyter"/><summary type="html"><![CDATA[An article originally published in ECarrier Magazine 2024 April Issue]]></summary></entry></feed>