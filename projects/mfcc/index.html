<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MFCC Speaker Recognition | Devnith Wijesinghe </title> <meta name="author" content="Devnith Wijesinghe"> <meta name="description" content="Speaker Recognition model based on Mel Frequency Cepstral Coefficients"> <meta name="keywords" content="devnith, academic, machine-learning, deep-learning, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://devnithw.github.io/projects/mfcc/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Devnith </span> Wijesinghe </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MFCC Speaker Recognition</h1> <p class="post-description">Speaker Recognition model based on Mel Frequency Cepstral Coefficients</p> </header> <article> <blockquote> <p>This project is an approach we used for our solution at Signal Processing Cup 2024. Full solution code by Team Eigensharks can be found <a href="https://github.com/eigensharks/Eigensharks" rel="external nofollow noopener" target="_blank">here</a>.</p> </blockquote> <h2 id="robovox-challenge---sp-sup-2024">Robovox Challenge - SP Sup 2024</h2> <p>The primary task of Signal Processing Cup 2024 was Far-Field Speaker Recognition by a Mobile Robot. During this task we had to accurately classify noisy audio recordings of 71 speakers. Our solution contain mainly two steps; denoising the audio files and classification. After denoising and preprocessing stages (using Wavelet transform etc.), we used several deep learning techniques and models to create a speaker embedding.</p> <p>Read more about the competition <a href="https://signalprocessingsociety.org/sites/default/files/uploads/community_involvement/docs/2024_spcup_official_doc.pdf" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="about-the-dataset">About the Dataset</h2> <p>The Robovox dataset features recordings from a mobile robot equipped with speaker recognition capabilities, utilizing a loudspeaker located beneath it to communicate. With contributions from 78 speakers, the dataset comprises approximately 11,000 recorded dialogues, generated through 2,219 conversations, each typically consisting of 5 speaker turns. These recordings, averaging 3.6 seconds in length, are captured in various acoustic settings and are available in 8-channel format.</p> <h2 id="mfcc--deep-learning-approach">MFCC + Deep Learning approach</h2> <p>One approach of our solution applied the technique of extracting MFCC from the audio samples and feeding them into a Deep Neural Network (DNN) for training. When a <a href="https://www.linkedin.com/pulse/exploring-librosa-comprehensive-guide-audio-feature-extraction-m/" rel="external nofollow noopener" target="_blank">feature extractor</a> of an speech audio sample (such as STFT or MFCC) is fed into a DNN model, it learns to create a feature space, known as an embedding, from several such samples of audio. That is, when given a unique voice recording of a speaker, the model maps it to a certain point on a feature space (similar to a vector space) unique to that speaker. We can force the model to map recordings of similar voices to the same point, by selecting an appropriate loss function. During our implementation we architectured a popular DNN structure known as a <code class="language-plaintext highlighter-rouge">bidirectional LSTM</code> with <code class="language-plaintext highlighter-rouge">Triplet Cosine Loss</code> as the loss function. This loss function relies on <code class="language-plaintext highlighter-rouge">cosine distance</code>, a popular metric of similarity in a vector embedding context. Using the speaker embedding we formulated using this approach, we were able to inference accurately on new data, to classify which speaker each recording belonged to.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/mfcc_comp.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="blood sample image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Read more about MFCCs <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="mel-frequency-cepstral-coefficients">Mel-Frequency Cepstral Coefficients</h2> <p>It was discovered that by analyzing the spectrum of a spectrum (Known as the cepstrum), the periodic structures within the frequency spectrum can be revealed, offering insights into the low-frequency periodic vibrations from vocal cords and the formant filtering effects in the larynx. Cepstral coefficients can be calculated as follows,</p> \[C_p = |\mathcal{F}\{log(|\mathcal{F}\{f(t)\}|^2)\}|^2\] <p>By adopting the Mel-scale during log-scaling stage, improves sensitivity to human voice. By employing the Mel-frequency cepstrum, derived from scaling spectrum coefficients with the Mel-scale, a more accurate representation of vocal characteristics can be achieved. The calculation process involves utilizing a triangular filter-bank and various technical considerations, but it can be calculated in one line of code in Python using <code class="language-plaintext highlighter-rouge">Librosa</code> library.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">librosa</span>
<span class="kn">import</span> <span class="n">librosa.display</span>

<span class="c1"># Load audio waveform
</span><span class="n">x</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">audio.wav</span><span class="sh">"</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="c1"># Calculate MFCC spectrum
</span><span class="n">mfccs</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="nf">mfcc</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">n_mfcc</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
</code></pre></div></div> <p>MFCC obtained for one example recording as such is shown below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/mfcc_graph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="blood sample image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="file-structure">File Structure</h2> <ul> <li> <code class="language-plaintext highlighter-rouge">model.py</code> contains the code for LSTM model.</li> <li> <code class="language-plaintext highlighter-rouge">loss.py</code> contains the Triplet Cosine Loss funtion definition.</li> <li> <code class="language-plaintext highlighter-rouge">feature_extraction.py</code> contains the helper function code to obtian MFCC coefficients during feature extraction.</li> <li> <code class="language-plaintext highlighter-rouge">data.py</code> contains the code for structuring, preprocessing and loading the ROBOVOX dataset.</li> <li> <code class="language-plaintext highlighter-rouge">train.py</code> contains the code for training the model.</li> <li> <code class="language-plaintext highlighter-rouge">visualize_data.ipynb</code> is a Jupyter Notebook which explores the dataset by observing STFT, MFCC and other features of the audio data.</li> </ul> <h2 id="technologies-used">Technologies used</h2> <ul> <li>PyTorch</li> <li>Librosa</li> <li>Google Colab</li> </ul> <h2 id="other">Other</h2> <p>Full solution code: <a href="https://github.com/eigensharks/Eigensharks" rel="external nofollow noopener" target="_blank">https://github.com/eigensharks/Eigensharks</a></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Devnith Wijesinghe. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>